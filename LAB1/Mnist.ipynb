{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module) : \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates (read more about hyperparameter tuning)\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "Number of Epochs - the number times to iterate over the dataset\n",
    "\n",
    "Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
    "\n",
    "Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309846  [   64/60000]\n",
      "loss: 2.262926  [ 6464/60000]\n",
      "loss: 2.226668  [12864/60000]\n",
      "loss: 2.022470  [19264/60000]\n",
      "loss: 1.873692  [25664/60000]\n",
      "loss: 1.588073  [32064/60000]\n",
      "loss: 1.185797  [38464/60000]\n",
      "loss: 1.106651  [44864/60000]\n",
      "loss: 0.867868  [51264/60000]\n",
      "loss: 0.705015  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.669403 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.750323  [   64/60000]\n",
      "loss: 0.570050  [ 6464/60000]\n",
      "loss: 0.558776  [12864/60000]\n",
      "loss: 0.528733  [19264/60000]\n",
      "loss: 0.468419  [25664/60000]\n",
      "loss: 0.459662  [32064/60000]\n",
      "loss: 0.327592  [38464/60000]\n",
      "loss: 0.506855  [44864/60000]\n",
      "loss: 0.473619  [51264/60000]\n",
      "loss: 0.456398  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.393575 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.421146  [   64/60000]\n",
      "loss: 0.328039  [ 6464/60000]\n",
      "loss: 0.327742  [12864/60000]\n",
      "loss: 0.406460  [19264/60000]\n",
      "loss: 0.326067  [25664/60000]\n",
      "loss: 0.379341  [32064/60000]\n",
      "loss: 0.224743  [38464/60000]\n",
      "loss: 0.427649  [44864/60000]\n",
      "loss: 0.391567  [51264/60000]\n",
      "loss: 0.417036  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.332186 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.316483  [   64/60000]\n",
      "loss: 0.277370  [ 6464/60000]\n",
      "loss: 0.250670  [12864/60000]\n",
      "loss: 0.372238  [19264/60000]\n",
      "loss: 0.273487  [25664/60000]\n",
      "loss: 0.342505  [32064/60000]\n",
      "loss: 0.190748  [38464/60000]\n",
      "loss: 0.391703  [44864/60000]\n",
      "loss: 0.344553  [51264/60000]\n",
      "loss: 0.393988  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.301001 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.260804  [   64/60000]\n",
      "loss: 0.256586  [ 6464/60000]\n",
      "loss: 0.209665  [12864/60000]\n",
      "loss: 0.353398  [19264/60000]\n",
      "loss: 0.240328  [25664/60000]\n",
      "loss: 0.317442  [32064/60000]\n",
      "loss: 0.173504  [38464/60000]\n",
      "loss: 0.368715  [44864/60000]\n",
      "loss: 0.307990  [51264/60000]\n",
      "loss: 0.374194  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.278807 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.223046  [   64/60000]\n",
      "loss: 0.242698  [ 6464/60000]\n",
      "loss: 0.183236  [12864/60000]\n",
      "loss: 0.336512  [19264/60000]\n",
      "loss: 0.215536  [25664/60000]\n",
      "loss: 0.300422  [32064/60000]\n",
      "loss: 0.161600  [38464/60000]\n",
      "loss: 0.350749  [44864/60000]\n",
      "loss: 0.278083  [51264/60000]\n",
      "loss: 0.355012  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.260703 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.194155  [   64/60000]\n",
      "loss: 0.229945  [ 6464/60000]\n",
      "loss: 0.165086  [12864/60000]\n",
      "loss: 0.320963  [19264/60000]\n",
      "loss: 0.196064  [25664/60000]\n",
      "loss: 0.286904  [32064/60000]\n",
      "loss: 0.151313  [38464/60000]\n",
      "loss: 0.333852  [44864/60000]\n",
      "loss: 0.252438  [51264/60000]\n",
      "loss: 0.335735  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.244838 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.172136  [   64/60000]\n",
      "loss: 0.217821  [ 6464/60000]\n",
      "loss: 0.151051  [12864/60000]\n",
      "loss: 0.306686  [19264/60000]\n",
      "loss: 0.180191  [25664/60000]\n",
      "loss: 0.274842  [32064/60000]\n",
      "loss: 0.141922  [38464/60000]\n",
      "loss: 0.316670  [44864/60000]\n",
      "loss: 0.229278  [51264/60000]\n",
      "loss: 0.318213  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.230502 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.155651  [   64/60000]\n",
      "loss: 0.207151  [ 6464/60000]\n",
      "loss: 0.139404  [12864/60000]\n",
      "loss: 0.293308  [19264/60000]\n",
      "loss: 0.167553  [25664/60000]\n",
      "loss: 0.265744  [32064/60000]\n",
      "loss: 0.132844  [38464/60000]\n",
      "loss: 0.299811  [44864/60000]\n",
      "loss: 0.209840  [51264/60000]\n",
      "loss: 0.302291  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.217373 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.141643  [   64/60000]\n",
      "loss: 0.198448  [ 6464/60000]\n",
      "loss: 0.129566  [12864/60000]\n",
      "loss: 0.280612  [19264/60000]\n",
      "loss: 0.156772  [25664/60000]\n",
      "loss: 0.257755  [32064/60000]\n",
      "loss: 0.124407  [38464/60000]\n",
      "loss: 0.282857  [44864/60000]\n",
      "loss: 0.193547  [51264/60000]\n",
      "loss: 0.288985  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.205409 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.129876  [   64/60000]\n",
      "loss: 0.190289  [ 6464/60000]\n",
      "loss: 0.121192  [12864/60000]\n",
      "loss: 0.268185  [19264/60000]\n",
      "loss: 0.147447  [25664/60000]\n",
      "loss: 0.249892  [32064/60000]\n",
      "loss: 0.116513  [38464/60000]\n",
      "loss: 0.266905  [44864/60000]\n",
      "loss: 0.181373  [51264/60000]\n",
      "loss: 0.277302  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.194479 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.120022  [   64/60000]\n",
      "loss: 0.182934  [ 6464/60000]\n",
      "loss: 0.113738  [12864/60000]\n",
      "loss: 0.256351  [19264/60000]\n",
      "loss: 0.139209  [25664/60000]\n",
      "loss: 0.242347  [32064/60000]\n",
      "loss: 0.109364  [38464/60000]\n",
      "loss: 0.252408  [44864/60000]\n",
      "loss: 0.172282  [51264/60000]\n",
      "loss: 0.267361  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.184589 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.111259  [   64/60000]\n",
      "loss: 0.175914  [ 6464/60000]\n",
      "loss: 0.107311  [12864/60000]\n",
      "loss: 0.245737  [19264/60000]\n",
      "loss: 0.132113  [25664/60000]\n",
      "loss: 0.233521  [32064/60000]\n",
      "loss: 0.102782  [38464/60000]\n",
      "loss: 0.238882  [44864/60000]\n",
      "loss: 0.164358  [51264/60000]\n",
      "loss: 0.258846  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.175520 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.103559  [   64/60000]\n",
      "loss: 0.169254  [ 6464/60000]\n",
      "loss: 0.101484  [12864/60000]\n",
      "loss: 0.235725  [19264/60000]\n",
      "loss: 0.125663  [25664/60000]\n",
      "loss: 0.225096  [32064/60000]\n",
      "loss: 0.096497  [38464/60000]\n",
      "loss: 0.226307  [44864/60000]\n",
      "loss: 0.158079  [51264/60000]\n",
      "loss: 0.250761  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.167248 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.096361  [   64/60000]\n",
      "loss: 0.162995  [ 6464/60000]\n",
      "loss: 0.096359  [12864/60000]\n",
      "loss: 0.226295  [19264/60000]\n",
      "loss: 0.119342  [25664/60000]\n",
      "loss: 0.217718  [32064/60000]\n",
      "loss: 0.090853  [38464/60000]\n",
      "loss: 0.215377  [44864/60000]\n",
      "loss: 0.152573  [51264/60000]\n",
      "loss: 0.242792  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.159776 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.090071  [   64/60000]\n",
      "loss: 0.157311  [ 6464/60000]\n",
      "loss: 0.092200  [12864/60000]\n",
      "loss: 0.217434  [19264/60000]\n",
      "loss: 0.113712  [25664/60000]\n",
      "loss: 0.210284  [32064/60000]\n",
      "loss: 0.085860  [38464/60000]\n",
      "loss: 0.205190  [44864/60000]\n",
      "loss: 0.147211  [51264/60000]\n",
      "loss: 0.235405  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.4%, Avg loss: 0.152898 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.084219  [   64/60000]\n",
      "loss: 0.151715  [ 6464/60000]\n",
      "loss: 0.088399  [12864/60000]\n",
      "loss: 0.208187  [19264/60000]\n",
      "loss: 0.108672  [25664/60000]\n",
      "loss: 0.203091  [32064/60000]\n",
      "loss: 0.081508  [38464/60000]\n",
      "loss: 0.196060  [44864/60000]\n",
      "loss: 0.142471  [51264/60000]\n",
      "loss: 0.228440  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.146596 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.078877  [   64/60000]\n",
      "loss: 0.145963  [ 6464/60000]\n",
      "loss: 0.084664  [12864/60000]\n",
      "loss: 0.199608  [19264/60000]\n",
      "loss: 0.104166  [25664/60000]\n",
      "loss: 0.195974  [32064/60000]\n",
      "loss: 0.077400  [38464/60000]\n",
      "loss: 0.187007  [44864/60000]\n",
      "loss: 0.138445  [51264/60000]\n",
      "loss: 0.221630  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.140783 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.074016  [   64/60000]\n",
      "loss: 0.140813  [ 6464/60000]\n",
      "loss: 0.081402  [12864/60000]\n",
      "loss: 0.190973  [19264/60000]\n",
      "loss: 0.099891  [25664/60000]\n",
      "loss: 0.189118  [32064/60000]\n",
      "loss: 0.073629  [38464/60000]\n",
      "loss: 0.178667  [44864/60000]\n",
      "loss: 0.134998  [51264/60000]\n",
      "loss: 0.215209  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.135385 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.069605  [   64/60000]\n",
      "loss: 0.135872  [ 6464/60000]\n",
      "loss: 0.078420  [12864/60000]\n",
      "loss: 0.182131  [19264/60000]\n",
      "loss: 0.095866  [25664/60000]\n",
      "loss: 0.181987  [32064/60000]\n",
      "loss: 0.070357  [38464/60000]\n",
      "loss: 0.170733  [44864/60000]\n",
      "loss: 0.131743  [51264/60000]\n",
      "loss: 0.208983  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.130429 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.065807  [   64/60000]\n",
      "loss: 0.131545  [ 6464/60000]\n",
      "loss: 0.075897  [12864/60000]\n",
      "loss: 0.173232  [19264/60000]\n",
      "loss: 0.091921  [25664/60000]\n",
      "loss: 0.175569  [32064/60000]\n",
      "loss: 0.067342  [38464/60000]\n",
      "loss: 0.163321  [44864/60000]\n",
      "loss: 0.128696  [51264/60000]\n",
      "loss: 0.202214  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.125892 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.062422  [   64/60000]\n",
      "loss: 0.127529  [ 6464/60000]\n",
      "loss: 0.073801  [12864/60000]\n",
      "loss: 0.164816  [19264/60000]\n",
      "loss: 0.088040  [25664/60000]\n",
      "loss: 0.169219  [32064/60000]\n",
      "loss: 0.064491  [38464/60000]\n",
      "loss: 0.156270  [44864/60000]\n",
      "loss: 0.125693  [51264/60000]\n",
      "loss: 0.195819  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.121706 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.059308  [   64/60000]\n",
      "loss: 0.123428  [ 6464/60000]\n",
      "loss: 0.071609  [12864/60000]\n",
      "loss: 0.157048  [19264/60000]\n",
      "loss: 0.084641  [25664/60000]\n",
      "loss: 0.163378  [32064/60000]\n",
      "loss: 0.061948  [38464/60000]\n",
      "loss: 0.149790  [44864/60000]\n",
      "loss: 0.123182  [51264/60000]\n",
      "loss: 0.189729  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.117840 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.056425  [   64/60000]\n",
      "loss: 0.119278  [ 6464/60000]\n",
      "loss: 0.069893  [12864/60000]\n",
      "loss: 0.149533  [19264/60000]\n",
      "loss: 0.081720  [25664/60000]\n",
      "loss: 0.156934  [32064/60000]\n",
      "loss: 0.059721  [38464/60000]\n",
      "loss: 0.143991  [44864/60000]\n",
      "loss: 0.120918  [51264/60000]\n",
      "loss: 0.183917  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.114221 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.053382  [   64/60000]\n",
      "loss: 0.115591  [ 6464/60000]\n",
      "loss: 0.068228  [12864/60000]\n",
      "loss: 0.142248  [19264/60000]\n",
      "loss: 0.078953  [25664/60000]\n",
      "loss: 0.151257  [32064/60000]\n",
      "loss: 0.057770  [38464/60000]\n",
      "loss: 0.138409  [44864/60000]\n",
      "loss: 0.118764  [51264/60000]\n",
      "loss: 0.177933  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.110901 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.050589  [   64/60000]\n",
      "loss: 0.112493  [ 6464/60000]\n",
      "loss: 0.066665  [12864/60000]\n",
      "loss: 0.135012  [19264/60000]\n",
      "loss: 0.076032  [25664/60000]\n",
      "loss: 0.145263  [32064/60000]\n",
      "loss: 0.056047  [38464/60000]\n",
      "loss: 0.133110  [44864/60000]\n",
      "loss: 0.116701  [51264/60000]\n",
      "loss: 0.172458  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.107807 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.047923  [   64/60000]\n",
      "loss: 0.109524  [ 6464/60000]\n",
      "loss: 0.065373  [12864/60000]\n",
      "loss: 0.128508  [19264/60000]\n",
      "loss: 0.073288  [25664/60000]\n",
      "loss: 0.139258  [32064/60000]\n",
      "loss: 0.054495  [38464/60000]\n",
      "loss: 0.128036  [44864/60000]\n",
      "loss: 0.114760  [51264/60000]\n",
      "loss: 0.167017  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.104895 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.045573  [   64/60000]\n",
      "loss: 0.106744  [ 6464/60000]\n",
      "loss: 0.063937  [12864/60000]\n",
      "loss: 0.122203  [19264/60000]\n",
      "loss: 0.070136  [25664/60000]\n",
      "loss: 0.133231  [32064/60000]\n",
      "loss: 0.052998  [38464/60000]\n",
      "loss: 0.123341  [44864/60000]\n",
      "loss: 0.112952  [51264/60000]\n",
      "loss: 0.161932  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.102139 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.043600  [   64/60000]\n",
      "loss: 0.103737  [ 6464/60000]\n",
      "loss: 0.062418  [12864/60000]\n",
      "loss: 0.116152  [19264/60000]\n",
      "loss: 0.066861  [25664/60000]\n",
      "loss: 0.127371  [32064/60000]\n",
      "loss: 0.051560  [38464/60000]\n",
      "loss: 0.118793  [44864/60000]\n",
      "loss: 0.111251  [51264/60000]\n",
      "loss: 0.157245  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.099620 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.041749  [   64/60000]\n",
      "loss: 0.101012  [ 6464/60000]\n",
      "loss: 0.060881  [12864/60000]\n",
      "loss: 0.110386  [19264/60000]\n",
      "loss: 0.064103  [25664/60000]\n",
      "loss: 0.121763  [32064/60000]\n",
      "loss: 0.050215  [38464/60000]\n",
      "loss: 0.114615  [44864/60000]\n",
      "loss: 0.109363  [51264/60000]\n",
      "loss: 0.152496  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.097225 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.040152  [   64/60000]\n",
      "loss: 0.097994  [ 6464/60000]\n",
      "loss: 0.059661  [12864/60000]\n",
      "loss: 0.105191  [19264/60000]\n",
      "loss: 0.061109  [25664/60000]\n",
      "loss: 0.116178  [32064/60000]\n",
      "loss: 0.049015  [38464/60000]\n",
      "loss: 0.110737  [44864/60000]\n",
      "loss: 0.107605  [51264/60000]\n",
      "loss: 0.147633  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.094968 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.038714  [   64/60000]\n",
      "loss: 0.094916  [ 6464/60000]\n",
      "loss: 0.058343  [12864/60000]\n",
      "loss: 0.099977  [19264/60000]\n",
      "loss: 0.058032  [25664/60000]\n",
      "loss: 0.110643  [32064/60000]\n",
      "loss: 0.047948  [38464/60000]\n",
      "loss: 0.106947  [44864/60000]\n",
      "loss: 0.106114  [51264/60000]\n",
      "loss: 0.143311  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.092948 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.037406  [   64/60000]\n",
      "loss: 0.092238  [ 6464/60000]\n",
      "loss: 0.057154  [12864/60000]\n",
      "loss: 0.095049  [19264/60000]\n",
      "loss: 0.055081  [25664/60000]\n",
      "loss: 0.105302  [32064/60000]\n",
      "loss: 0.046881  [38464/60000]\n",
      "loss: 0.103149  [44864/60000]\n",
      "loss: 0.104473  [51264/60000]\n",
      "loss: 0.138744  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.090983 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.036185  [   64/60000]\n",
      "loss: 0.089506  [ 6464/60000]\n",
      "loss: 0.055830  [12864/60000]\n",
      "loss: 0.090564  [19264/60000]\n",
      "loss: 0.052441  [25664/60000]\n",
      "loss: 0.100359  [32064/60000]\n",
      "loss: 0.045850  [38464/60000]\n",
      "loss: 0.099648  [44864/60000]\n",
      "loss: 0.102991  [51264/60000]\n",
      "loss: 0.134095  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.089160 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.034884  [   64/60000]\n",
      "loss: 0.086889  [ 6464/60000]\n",
      "loss: 0.054548  [12864/60000]\n",
      "loss: 0.086003  [19264/60000]\n",
      "loss: 0.049882  [25664/60000]\n",
      "loss: 0.095619  [32064/60000]\n",
      "loss: 0.044927  [38464/60000]\n",
      "loss: 0.096211  [44864/60000]\n",
      "loss: 0.101412  [51264/60000]\n",
      "loss: 0.129399  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.087441 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.033889  [   64/60000]\n",
      "loss: 0.084043  [ 6464/60000]\n",
      "loss: 0.053506  [12864/60000]\n",
      "loss: 0.081872  [19264/60000]\n",
      "loss: 0.047564  [25664/60000]\n",
      "loss: 0.091092  [32064/60000]\n",
      "loss: 0.044101  [38464/60000]\n",
      "loss: 0.092837  [44864/60000]\n",
      "loss: 0.100195  [51264/60000]\n",
      "loss: 0.124731  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.085821 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.033002  [   64/60000]\n",
      "loss: 0.081474  [ 6464/60000]\n",
      "loss: 0.052344  [12864/60000]\n",
      "loss: 0.078470  [19264/60000]\n",
      "loss: 0.045260  [25664/60000]\n",
      "loss: 0.086619  [32064/60000]\n",
      "loss: 0.043155  [38464/60000]\n",
      "loss: 0.089711  [44864/60000]\n",
      "loss: 0.099168  [51264/60000]\n",
      "loss: 0.120125  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.084325 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.032080  [   64/60000]\n",
      "loss: 0.078798  [ 6464/60000]\n",
      "loss: 0.051083  [12864/60000]\n",
      "loss: 0.074707  [19264/60000]\n",
      "loss: 0.043147  [25664/60000]\n",
      "loss: 0.082211  [32064/60000]\n",
      "loss: 0.042338  [38464/60000]\n",
      "loss: 0.086557  [44864/60000]\n",
      "loss: 0.098188  [51264/60000]\n",
      "loss: 0.116125  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.082906 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.031187  [   64/60000]\n",
      "loss: 0.076264  [ 6464/60000]\n",
      "loss: 0.050035  [12864/60000]\n",
      "loss: 0.070866  [19264/60000]\n",
      "loss: 0.041105  [25664/60000]\n",
      "loss: 0.078084  [32064/60000]\n",
      "loss: 0.041491  [38464/60000]\n",
      "loss: 0.083545  [44864/60000]\n",
      "loss: 0.097198  [51264/60000]\n",
      "loss: 0.112241  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.081551 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.030262  [   64/60000]\n",
      "loss: 0.073499  [ 6464/60000]\n",
      "loss: 0.048944  [12864/60000]\n",
      "loss: 0.067561  [19264/60000]\n",
      "loss: 0.039029  [25664/60000]\n",
      "loss: 0.074165  [32064/60000]\n",
      "loss: 0.040598  [38464/60000]\n",
      "loss: 0.080511  [44864/60000]\n",
      "loss: 0.096186  [51264/60000]\n",
      "loss: 0.108469  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.080332 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.029669  [   64/60000]\n",
      "loss: 0.071147  [ 6464/60000]\n",
      "loss: 0.047942  [12864/60000]\n",
      "loss: 0.064229  [19264/60000]\n",
      "loss: 0.037070  [25664/60000]\n",
      "loss: 0.070588  [32064/60000]\n",
      "loss: 0.039943  [38464/60000]\n",
      "loss: 0.077757  [44864/60000]\n",
      "loss: 0.095011  [51264/60000]\n",
      "loss: 0.104568  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.079144 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.029016  [   64/60000]\n",
      "loss: 0.068699  [ 6464/60000]\n",
      "loss: 0.046794  [12864/60000]\n",
      "loss: 0.060776  [19264/60000]\n",
      "loss: 0.035310  [25664/60000]\n",
      "loss: 0.067396  [32064/60000]\n",
      "loss: 0.039189  [38464/60000]\n",
      "loss: 0.074913  [44864/60000]\n",
      "loss: 0.094151  [51264/60000]\n",
      "loss: 0.100913  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.078033 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.028477  [   64/60000]\n",
      "loss: 0.066391  [ 6464/60000]\n",
      "loss: 0.045593  [12864/60000]\n",
      "loss: 0.057363  [19264/60000]\n",
      "loss: 0.033615  [25664/60000]\n",
      "loss: 0.064075  [32064/60000]\n",
      "loss: 0.038519  [38464/60000]\n",
      "loss: 0.072221  [44864/60000]\n",
      "loss: 0.093193  [51264/60000]\n",
      "loss: 0.097420  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.077015 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.027969  [   64/60000]\n",
      "loss: 0.064143  [ 6464/60000]\n",
      "loss: 0.044428  [12864/60000]\n",
      "loss: 0.054411  [19264/60000]\n",
      "loss: 0.032058  [25664/60000]\n",
      "loss: 0.060968  [32064/60000]\n",
      "loss: 0.037731  [38464/60000]\n",
      "loss: 0.069292  [44864/60000]\n",
      "loss: 0.092006  [51264/60000]\n",
      "loss: 0.093807  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.076069 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.027519  [   64/60000]\n",
      "loss: 0.061787  [ 6464/60000]\n",
      "loss: 0.043368  [12864/60000]\n",
      "loss: 0.050962  [19264/60000]\n",
      "loss: 0.030647  [25664/60000]\n",
      "loss: 0.058325  [32064/60000]\n",
      "loss: 0.036978  [38464/60000]\n",
      "loss: 0.066700  [44864/60000]\n",
      "loss: 0.091014  [51264/60000]\n",
      "loss: 0.090345  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.075156 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.026998  [   64/60000]\n",
      "loss: 0.059758  [ 6464/60000]\n",
      "loss: 0.042212  [12864/60000]\n",
      "loss: 0.048005  [19264/60000]\n",
      "loss: 0.029395  [25664/60000]\n",
      "loss: 0.055584  [32064/60000]\n",
      "loss: 0.036235  [38464/60000]\n",
      "loss: 0.064124  [44864/60000]\n",
      "loss: 0.090176  [51264/60000]\n",
      "loss: 0.087018  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.074319 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.026582  [   64/60000]\n",
      "loss: 0.057764  [ 6464/60000]\n",
      "loss: 0.041371  [12864/60000]\n",
      "loss: 0.045321  [19264/60000]\n",
      "loss: 0.028238  [25664/60000]\n",
      "loss: 0.053220  [32064/60000]\n",
      "loss: 0.035454  [38464/60000]\n",
      "loss: 0.061770  [44864/60000]\n",
      "loss: 0.089565  [51264/60000]\n",
      "loss: 0.083921  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.073521 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.026118  [   64/60000]\n",
      "loss: 0.055513  [ 6464/60000]\n",
      "loss: 0.040413  [12864/60000]\n",
      "loss: 0.042787  [19264/60000]\n",
      "loss: 0.027089  [25664/60000]\n",
      "loss: 0.050955  [32064/60000]\n",
      "loss: 0.034561  [38464/60000]\n",
      "loss: 0.059064  [44864/60000]\n",
      "loss: 0.088805  [51264/60000]\n",
      "loss: 0.080607  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.072786 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.025902  [   64/60000]\n",
      "loss: 0.053624  [ 6464/60000]\n",
      "loss: 0.039664  [12864/60000]\n",
      "loss: 0.040567  [19264/60000]\n",
      "loss: 0.025881  [25664/60000]\n",
      "loss: 0.048592  [32064/60000]\n",
      "loss: 0.033794  [38464/60000]\n",
      "loss: 0.056620  [44864/60000]\n",
      "loss: 0.087592  [51264/60000]\n",
      "loss: 0.077499  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.072055 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.025488  [   64/60000]\n",
      "loss: 0.051723  [ 6464/60000]\n",
      "loss: 0.038778  [12864/60000]\n",
      "loss: 0.037903  [19264/60000]\n",
      "loss: 0.024907  [25664/60000]\n",
      "loss: 0.046436  [32064/60000]\n",
      "loss: 0.032975  [38464/60000]\n",
      "loss: 0.054279  [44864/60000]\n",
      "loss: 0.087096  [51264/60000]\n",
      "loss: 0.074655  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071399 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
